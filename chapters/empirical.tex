%%
%% Author: jamie
%% 04/10/18
%%

% Document
\chapter{Empirical Studies}\label{ch:empirical}

\section{Overview}\label{sec:empOverview}
In this chapter, we will cover a number of experiments that were conducted in order to investigate
the performance of the different algorithms implemented in order to tackle Leduc Hold'em.
We will begin with a simplified version of MCTS for POMDPs and will incrementally add to this
method in order to see how the performance of our agent evolves.
In the table below we have listed the template that we will follow when conduction these experiments.


\addvbuffer[12pt 0pt]{\begin{tabular}[ht]{ | p{2cm} | p{10cm} | }
    \hline
    \textbf{Section} & \textbf{Rationale} \\
    \hline
    Objective & This section will contain an explanation of the purpose of the experiment along with
    how it was carried out \\
    \hline
    Experimental Parameters & This section outlines how we parameterised the experiment. \\
    \hline
    Results & This section will detail the results acquired from the experiments conducted \\
    \hline
    Analysis & In this section we will examine our results and try to provide insight into the
    reasoning behind these results \\
    \hline
\end{tabular}}

\section{Experiment 1 - UCT Versus Random Player}\label{sec:expmeriment1}
The first experiment conducted involved a simplified version of the algorithm outlined in\citep{heinrich2017reinforcement}.
We set an initial goal of using a random player as our benchmark opponent in order to demonstrate how
this algorithm could exploit such a player's strategic inefficiencies.

\subsection{Objective}\label{subsec:objective1}
The goal of this experiment is to implement UCT for Leduc Hold'em.
The UCT agent will play against a random player and learn a strategy to exploit this player for
maximal reward.
Although we are interested in the results gained from playing against the random player we treat the
outcome of this experiment as a baseline for our subsequent results.
The reasoning for this is that the difficulty in finding a winning strategy against a random player
is not high.
In fact, at each point in the game that the random player can take an action it is
just as likely to fold it's cards, regardless of their value, as it is to take any other action.
This means that for our agent the intelligent strategy is to merely retain all but the
very worst of its hands.
Thus we can think of the strategy learned by this initial agent as a broad
categorisation between very weak hands and all other hands.
Thus we expect that the exploitability of the resultant agent will be relatively high.
Our agent will not have learned all of the strategic intricacies of the game
and it will not react strategically to the opponent's play.
Rather, it will simply know how to beat a 'dumb' random player.
This will give us a platform to build a more sophisticated agent through different mechanisms such as
self-play in the subsequent experiments.

\subsection{Experimental Parameters}\label{subsec:algAndCoding1}
For each experiment, we will list the parameters that were used to instantiate the agent.
These parameters either refer to the execution time of the algorithm or are a part of
the group of parameters that were outlined in Algorithms 1, 2 and 3.
In the case of each experiment, we replicate the values outlined in\citep{heinrich2017reinforcement}.
If there is a deviation from these values the reasoning behind it will be explained.

Below are listed the key parameters for the first experiment:
\begin{itemize}
    \item \textbf{Iterations} - 500,000
    \item \textbf{Repetitions} - 20
    \item \textbf{$c$ - exploration constant} - 18
\end{itemize}

\subsection{Results}\label{subsec:results1}
The first metric that was used in order to analyse the results of this experiment is cumulative reward.
This is simply the sum of the output of the reward function over time.
This function directly corresponds to the size of the pot won or lost in the game,
thus the reward can be either positive or negative.
In figure 4.1 we see the reward over time increasing.
In order to obtain these results, the algorithm was run for 10,000 iterations and this process was repeated 100 times.
Our cumulative rewards were then averaged at each iteration across these 100 repetitions to give the graph shown.
Figure 4.2 shows the rate of increase in cumulative reward or the average reward over time.
In the case of figure 4.2, we applied the UCT algorithm for 500,000 iterations and
repeated this process 20 times, averaging the results.

\begin{figure}[ht]
    \includegraphics[scale=.8]{images/cumulative_reward_mcts_vs_random.png}
    \caption{UCT cumulative reward vs random player - 10000 Iterations}
\end{figure}

\begin{figure}[ht]
    \includegraphics[scale=.7]{images/avg_reward_500000__20_random.png}
    \caption{UCT average reward vs random player - 500000 Iterations}
\end{figure}

The second metric used to produce results was exploitability.
This is a measure of the reward that can be gained by playing a best response strategy
against the agent.
In figure 4.3 we see that the exploitability of our agent is quite high throughout, with an
initial dip followed by a divergence towards 4.9.

\begin{figure}[ht]
    \includegraphics[scale=.7]{images/exploitability_500000_20_random.png}
    \caption{UCT exploitability vs random player - 500000 Iterations}
\end{figure}

\subsection{Analysis}\label{subsec:analysis1}
We will first analyse our results from the cumulative reward metric.
As shown by figure 4.1 we see that initially there is a gradual increase in cumulative reward,
with the slope growing until there is a constant rate of increase.
This demonstrates that during the initial phase of the algorithm we have not yet uncovered
the most beneficial action selection for all states and the exploration phase is still in effect.
However, by visual inspection, we can see that over time the rate of increase in cumulative
reward begins to stabilise which indicates that a concrete strategy that can exploit
the random play has been established.
This hypothesis is further supported by figure 4.2 as we see a dramatic upswing
in the rate of increase of cumulative reward followed by a leveling of the graph.

As mentioned the exploitability value throughout this figure 4.3 is relatively poor with an initial dip followed
by a divergence.
This is most likely explained by the fact that we are not playing against a rational player.
As such our strategy is strong when it comes to maximally exploiting an irrational, random player
but if we then substitute this irrational player for a rational player, the results will not be favourable.

\section{Experiment 2 - UCT Self-Play} \label{sec:experiment2}
In our second experiment, the agent was trained against itself.
In other words, two instances of the agent were generated and allowed to develop strategies
through self-play.

\subsection{Objective}\label{subsec:objective2}
The objective of this experiment was to implement Heinrich's extensive form MCTS using UCB action
selection (ie extensive form UCT).
Here a significant improvement in the exploitability of the agent was expected.
This was due to the fact that the opposing agent is rational, in that it learns
through experience.


\subsection{Experimental Parameters}\label{subsec:algAndCoding2}
Below are listed the key parameters of the experiment
\begin{itemize}
    \item \textbf{Iterations} - 500,000
    \item \textbf{Repetitions} - 10
    \item \textbf{$c$ - exploration constant} - 18
\end{itemize}

\subsection{Results}\label{subsec:results2}
In the case of self-play both agents develop intelligent strategies.
This means that we do not see any significant trends in cumulative reward or average reward over time
due to the fact that neither player has an advantage.
This is shown in figure 4.4 as the average reward for player 1 undulates around 0.
\begin{figure}[ht]
    \includegraphics[scale=.7]{images/avg_winnings_self-play.png}
    \caption{UCT average reward - self-play - 500000 Iterations}
\end{figure}
As such these metrics were disregarded and exploitability became the sole metric.
Once again we applied the algorithm for 500,000 iterations, repeating this process 10 times
and averaging the results.

In figure 4.5 it can be seen that there is a notable improvement in exploitability, with lows of 3.5 initially
but over time the values begin to diverge once again back towards 4.6.

\begin{figure}[ht]
    \includegraphics[scale=.7]{images/exploitability_self-play_deterministic_500000.png}
    \caption{UCT exploitability - self play - 500000 Iterations}
\end{figure}

\subsection{Analysis}\label{subsec:analysis2}
The results gleaned from this experiment were unexpected to a degree.
Heinrich had shown exploitability reaching lows of .8 and diverging to 1.5 in a similar experiment.
Although a similar trend was shown here our exploitability values were significantly higher.
This could potentially be explained by differences in the algorithm used to calculate exploitability
or the implementation of UCT itself.

On further inspection of the search trees generated, along with the best response tree (see chapter 3)
there appeared to be an over-fitting to the opponent's strategy over time.
Notably, both instances of the agent were playing more conservatively than expected.
This meant that the best-response player could regularly force our agent to fold in
situations where folding against a less conservative player would be illogical.
In order to conceptualize this phenomenon, the following example is given.
Let's say player one is very conservative and they decide to only raise in the second round of the game when they
have a pair of aces.
This means that over time player two will only receive highly negative rewards for the states in
which player one has raised in the second round.
In fact, in this case, it is more beneficial for player two to simply fold if player one raises in
the second round of the game.
When a new player is introduced to the system, however, they can take advantage of this
behavior by simply raising more frequently in the second round of the game.
This type of strategic feedback loop is characteristic of deterministic strategies due to the
fact that breaking such a loop becomes less and less likely the closer we get to purely greedy action selection.


\section{Experiment 3 - Smooth UCT}\label{sec:experiment3}
The final experiment conducted during our empirical studies utilised both self-play
and average strategy sampling.
Ie smooth UCT was used.

\subsection{Objective}\label{subsec:objective3}
The objective of this experiment was to implement Smooth UCT as outlined in\citep{heinrich2017reinforcement}.
Again a significant improvement in exploitability was expected at this stage due to the fact
that stochastic strategies were now in use in the training phase.
Heinrich showed smooth UCT to converge to an approximate Nash's equilibrium for Leduc Hold'em.
As such the primary goal for this experiment is to replicate these results as closely as possible.
This goal was set with the caveat that, as mentioned in section 3.2.2 only deterministic strategies
could be evaluated for exploitability at this stage of development.
As such the exploitability values produced may be higher than expected.

\subsection{Experimental Parameters}\label{subsec:algAndCoding3}
Note that in the case of smooth UCT the $d$ parameter has been modified, with Heinrich's
original value being .002.
It was found through experimentation that Heinrich's stated value did not provide a significant
improvement compared to experiment 2's results.
However, when increased to .1 we saw our best exploitability results.

Below are listed the key parameters of the experiment:
\begin{itemize}
    \item \textbf{Iterations} - 1,000,000
    \item \textbf{Repetitions} - 10
    \item \textbf{$c$ - exploration constant} - 18
    \item \textbf{$\gamma$} - .01
    \item \textbf{$\eta_0$} - .9
    \item \textbf{$d$} - .1
\end{itemize}

\subsection{Results}\label{subsec:results3}
For this experiment exploitability was once again the sole metric used.
In this case, the algorithm was run for 1,000,000 iterations and 10 repetitions with the results averaged.
The results of the experiment are shown in figure 4.6.
\begin{figure}[!ht]
    \includegraphics[scale=.7]{images/exploitability_self-play_stochastic_1000000.png}
    \caption{Smooth UCT exploitability - self play - 1000000 Iterations}
\end{figure}

Here we see a significant improvement compared to the last experiment in which deterministic strategies were used.
There is no longer a tendency for exploitability to diverge as there it had in experiment 2 with
early exploitability values of 3.8 converging towards 2.2 and stabilising.
However, these values are still less favourable than those demonstrated by Heinrich who achieved
exploitability of roughly .5 after 1,000,000 iterations.

\subsection{Analysis}\label{subsec:analysis3}
The relative success of this experiment compared to the others demonstrates the value of
utilising stochastic strategies in these scenarios.
The over-fitting effect mentioned in experiment 2 was no longer an issue.
This is likely due to the fact that when the average strategy is being sampled there is a
chance to break out of such a feedback loop as all possible actions will be sampled over time.

Although Heinrich's benchmark was not met, there are a number of extenuating factors that must be
considered in order to contextualize these results.
Firstly, the starting exploitability value is significantly higher than Heinrich's (3.8 vs 2).
This may indicate that the general methodology for exploitability computation differed across these
two implementations.
Second, due to the scope of the project extensive parameter tuning was not possible.
It's most likely that these results could be significantly improved if the optimal parameter
combination was found for this implementation.
Third, as mentioned, only exploitability calculation for deterministic strategies was performed.
If this had been implemented for stochastic strategies then the exploitability values would
likely have been lower due to the fact that stochastic strategies fare far more favourably in
imperfect information games\citep{heinrich2016deep}.
Finally, the computational resources available for this project were limited and as we could not
fully replicate Heinrich's experiments.
It would have taken a number of days to complete 500,000,000 iterations of the algorithm on the
available hardware.