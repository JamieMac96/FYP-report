\chapter{Introduction}
\label{ch:intro}

\section{Overview}\label{sec:overview}
Since the inception of machine learning, games have been a key problem area that has seen a lot of focus from
top academics.
For decades games have been used as a platform to test and develop algorithms that have gone on to provide invaluable
services that are used in peoples everyday lives.
The ability to contribute to this great history was a large motivator when it came to choosing this project.

Although this report will, to an extent, discuss machine learning and how it applies to games in general, the
primary focus will be on machine learning techniques when applied to texas hold'em, or variations of it.
In the past, methods such as Counterfactual Regret Minimization (CFR) have been used to develop agents that can
play texas hold'em to a superhuman level.
CFR is an algorithm that allows computation of a strategy through self-play.
The metric used to update these strategies is called regret, which measures the difference between the
game's outcome and the outcome that could have been achieved if some other action was taken.
A large number of simulated games are played, with regret being calculated each time and subsequently being used
to compute a strategy that is optimal.
One example of CFR being used was the 2018 Annual Computer Poker Competition (ACPC) champion, slumbot\citep{jackson2013slumbot}.

When applied to large imperfect information games such as texas hold'em CFR generally relies on the
creation of abstractions of the game.
The CFR algorithm is applied to these abstractions, generating a strategy.
The generated strategy is then applied to the full version of the game and, if the
original abstraction was accurate, our strategy will work well.
This obviously requires a high degree of knowledge of the game in order to create an accurate abstraction.

There have also been attempts to tackle texas hold'em using a machine learning paradigm called reinforcement
learning (RL).
The great advantage of these approaches is that they do not require game abstractions or the associated
domain knowledge.

RL is a way of programming agents by reward and punishment without needing to specify how the
task is to be achieved\citep{kaelbling1996reinforcement}.
RL problems consist of an agent in an environment.
The environment consists of a number of states and rewards.
The agent is allowed to take certain actions, in certain states.
The overall goal is to learn a strategy that will maximise the cumulative reward.
On the surface this seems like a perfectly reasonable methodology for solving games such as texas hold'em.
However, texas hold'em is an imperfect information game.
This means that we do not know the entirety of the state information at any given time
i.e we do not know the values of the opponents cards.
Thus, from a RL perspective, we do not know the actual state from which we are choosing actions.
This makes pure reinforcement learning strategies impractical for these types of games.

However, there has been some success when more custom reinforcement learning methods have been implemented.
In one case linear programming techniques and RL were used in order to tackle a
simplified version of the game\citep{dahl2001reinforcement}.
In another RL was combined with techniques inspired by game theory\citep{heinrich2015fictitious}.
This latter approach will be the basis for our texas hold'em agent as we attempt to replicate and
build upon the results outlined in this paper.

The different approaches will be discussed in greater detail in the background section.

\section{Objectives}\label{sec:objectives}
\subsection{Primary Objectives}\label{subsec:primaryObjectives}

Although this project will be largely research based, the primary goal is to create a texas hold'em
playing agent.
Due to the fact that texas hold'em has an extremely large state space, combined with the fact that it
is an imperfect information game, the initial goal will be to tackle a simplified version of the game.
Specifically Leduc Hold'em will be used for this version of the project.
This version of hold'em consists usually of a six card deck and only one private card, compared to two in texas hold'em.

In\citep{heinrich2015fictitious} a metric called exploitability was used.
This is a measure of how the agent's strategy fares against the best responses to that strategy.
For Leduc Hold'em, with a 6 card deck and 300 seconds of training time the initial exploitability was slightly
over 1.2 and descended to under .5.
When the deck size increased to 60 cards the exploitability began at over 5.5 and converged to roughly 1.2 in
the same period of time.
As such the success criteria for our initial iteration of the game is to replicate these results, with an
allowance for hardware differences that may impact computational speed.

If the success criteria for this simplified version of the game are met, we will then proceed to tackle
a more complex version of the game.
In this second iteration of the project we will tackle limit texas hold'em with the end goal of recreating
the results shown by Heinrich in his second paper on the matter\citep{heinrich2016deep}.
In this paper, rather than using exploitability as an evaluation method, the agent was simply compared
against the best hold'em agents from the ACPC of the previous year.
Thus win rate was used as the evaluation metric.
More specifically the measure used was mbb/h or milli big-blinds per hand.
This is a measure of the number of big blinds won or lost per thousand hands.
Note that the big blind is the larger of the two mandatory bets required at the beginning of each hand.
Against the top three competitors in the ACPC, the fully trained agent achieved a win rate of between
-50 and -15 mbb/h.
This score is consistent with that of a player that is slightly inferior but still competitive with
these superhuman poker agents.
As such, if the second iteration of our project is completed the goal will be to have a win rate of -200mbb/h
or better against these same agents.
Again we give an allowance for the difference in hardware used to train the agent as well our limited time.

It is also my goal to create a product that will be fun and useful for the general public.
As such another objective will be to create a website that will allow users to play heads-up against the final product.

\subsection{Secondary Objectives}\label{subsec:secondaryObjectives}
As this project is very specific and academic, one of the larger challenges will be to gain a strong knowledge
of the domain.
This means learning the history of RL, the types of problems that it has been used to solve and the specific details of
different RL algorithms.

A successful project will require a high degree of knowledge from the broader domain of RL. However, it is also the case
that I must become closely familiar with the existing academic literature in the area of RL with respect to imperfect
information games.
This will allow me to avoid taking approaches that have previously shown to fail as well as allow me to add value to
the existing literature whether that be through literature review or through my own experimental findings.


\section{Contribution}\label{sec:contribution}

\section{Methodology}\label{sec:methodology}

\section{Motivation}\label{sec:Motivation}
For the last number of years I have played poker recreationally with friends or online.
It became more of an interest of mine as I started to explore the mathematical basis for the game and how
players could use their knowledge, intelligence and temperament in order to gain an advantage in a game that,
on the face of it, seemed to be largely based on chance.
I spent some of my free time researching different aspects of the game.
This included gaining some basic knowledge like the probability of making drawing hands as well as learning more
technical aspects of the game such as how to calculate expected value of hands,
or how to narrow down one's opponent's range of possible hands.

Concurrent with the development of this interest I was also becoming more and more interested in the area
of machine learning.
Machine learning and the development of artificial intelligence is possibly the most glamourized area
of computer science.
However, this is probably for good reason because there is something intrinsically interesting
about machines that can learn to solve a problem on their own, without direct instructions from a human.
The fact that machine learning has made so many strides in recent years was another cause of interest in
this area of computer science, especially as the practical viability of ML as means of tackling a wide array
of problems in industry continues to increase.

As a result the merging of these two interests as the basis for my final year project seemed like an obvious choice.