\chapter{Introduction}
\label{ch:intro}

\section{Overview}\label{sec:overview}
Since the inception of machine learning(ML), games have been a key problem area that has seen a lot of focus
in academia.
For decades games have been used as a platform to test and develop algorithms that have gone on to provide invaluable
services that are used in peoples everyday lives.
For example\citep{tesauro1995td} demonstrated the value of the reinforcement learning techniques
with his backgammon application.
Today we see these techniques starting to permeate complex industry domains such as vehicle
automation\citep{desjardins2011cooperative}.
The ability to contribute to this great history was a large motivator when it came to choosing this project.

The game that was initially selected for this project was Texas Hold'em which is a popular
variant of poker.
In chapters 1 and 2 we will primarily refer to Texas hold'em due to the
fact that the majority of the literature is concerned with solving this version of poker.
In chapters 3 and 4 we implement and analyse a solution for a simplified version of Texas
hold'em named Leduc hold'em.
As such we primarily refer to Leduc Hold'em in these chapters.

Although this report will, discuss ML and how it applies to games in general, the primary
focus will be on ML techniques when applied to Texas hold'em, or variations of it.
For the past number of years, a method called Counterfactual Regret Minimization (CFR) has dominated the
top spots in the Annual Computer Poker Competition(ACPC).
For example, the 2018 heads-up champion, named slumbot utilised these methods\citep{jackson2013slumbot}
CFR is an algorithm that allows computation of a strategy through self-play\citep{zinkevich2008regret}.
When applied to large imperfect information games such as Texas hold'em CFR generally relies on the
creation of abstractions of the game.
These abstractions are used to reduce the size of the game's state space.
CFR is then applied to these abstractions in order to learn a strategy.
It is thus critically important that these abstractions accurately represent the full game's mechanics.
This requires a high degree of knowledge of the game in order to create an accurate abstraction.

There have also been attempts to tackle Texas hold'em using a ML paradigm called reinforcement
learning (RL).
The great appeal of these approaches is that they do not require us to generate custom abstractions of the game.
As a result of this, it was decided that this project would utilise RL techniques to tackle the problem.

RL is a way of programming agents by reward and punishment without needing to specify how the
task is to be achieved\citep{kaelbling1996reinforcement}.
On a surface level, RL seems like a perfectly reasonable methodology for solving games such as Texas hold'em.
However, Texas hold'em is an imperfect information game.
This means that we do not know the entirety of the state information at any given time
i.e we do not know the values of the opponent's cards.
Thus, from a RL perspective, we do not know the actual state from which we are choosing actions.
This makes pure reinforcement learning strategies impractical for these types of games.

However, there has been some success when more custom reinforcement learning methods have been implemented.
In one case linear programming techniques and RL were used in order to tackle a
simplified version of the game\citep{dahl2001reinforcement}.
In another case, RL was combined with supervised learning and game theoretic
techniques\citep{heinrich2016deep} to develop an algorithm called Neural Fictitious Self-Play(NFSP).
This method became the focus of the project for a period of time before a decision was made
that the broad range of techniques involved would add too much risk to the project.
When it was discovered that the same researcher, Johannes Heinrich had applied an RL related search
algorithm called Monte Carlo Tree Search (MCTS) to a number of variations of poker\citep{heinrich2015smooth}
this algorithm became the primary focus of the project.
This algorithm was of great interest for a number of reasons.
Firstly, this relatively new technique had been the basis for the tremendous improvement in the
performance of computer Go programs over the last decade.
The culmination of which was the AlphaGo project that defeated the highest ranked human Go player in
the world\citep{silver2016mastering}.
Secondly, this approach had shown the best results of any of Heinrich's documented approaches
with the developed agent placing second in the 2014 ACPC .
Finally, the lead researcher of the AlphaGo project, David Silver had collaborated on Heinrich's
work thus adding significant weight to the research.


\section{Objectives}\label{sec:objectives}
\subsection{Primary Objectives}\label{subsec:primaryObjectives}

Although this project will be largely research-based, the initial goal is to create a Texas hold'em
playing agent.
Due to the fact that Texas hold'em has an extremely large state space, combined with the fact that it
is an imperfect information game, the initial goal will be to tackle a simplified version of the game.
Specifically, Leduc Hold'em will be used for this project.
This version of hold'em consists usually of a six card deck and only one private card, compared to two in Texas hold'em.

In\citep{heinrich2015smooth} a metric called exploitability was used.
This is a measure of how the agent's strategy fares against the best responses to that strategy.
In other words, if the opponent knows our strategy, and can take the best possible action
in every state in order to maximise their potential gain in reward, exploitability is the average amount
that they would gain from doing so.
For Leduc Hold'em, with a 6 card deck and 500,000,000 iterations, the initial exploitability was 2 and converged
to .0223.
As such the success criteria for our initial iteration of the game is to replicate these results, with an
allowance for hardware differences that may impact computational speed.

It is also my goal to create a product that will be fun and useful for the general public.
As such another objective will be to create an application that will allow users to play heads-up
against the final product.

\subsection{Secondary Objectives}\label{subsec:secondaryObjectives}
As this project is very specific and academic, one of the larger challenges will be to gain a strong knowledge
of the domain.
This means learning the history of RL, the types of problems that it has been used to solve and the specific details of
different RL algorithms.

A successful project will require a high degree of knowledge from the broader domain of RL. However, it is also the case
that I must become closely familiar with the existing academic literature in the area of RL with respect to imperfect
information games.
This will allow me to avoid taking approaches that have previously shown to fail as well as allow me to add value to
the existing literature whether that be through literature review or through my own experimental findings.


\section{Methodology}\label{sec:methodology}

The methodology for this project can be summarised as follows:

\begin{enumerate}
    \item \textbf{Identify Research Objectives}: The first step of this project was to define research goals.
    Although it was established that an agent for a variation of poker would be created early in the project,
    \item we did not yet know what variation we would be tackling nor the method that would
    be applied to do so.
    Identifying exactly what would be done and how was the first big step in this project.
    \item \textbf{Approximation of Systematic Literature Review}: As a result of the scope of the project a
    \item full-scale systematic literature review was not required.
    However, it was still important to gain a strong understanding of the relevant literature.
    This was achieved by utilising various search terms and in Google Scholar, saving
    any relevant papers and reviewing their contents.
    When particularly interesting papers were found many of their references were then
    followed and read to give a deeper understanding.
    \item \textbf{Identify Technique for Implementation}: When defining our research objectives the broad
    \item area of reinforcement learning had been selected but
    \item \textbf{Design and Implement Prototype}: When the pertinent research objectives had been
    defined and the method through which they would be achieved had been found, the next step
    was to design and implement the prototype.
    This involved closely following the algorithm defined in the chosen research paper and
    converting that to functional code.
    \item \textbf{Empirical Studies}: When the prototype had been developed the next step was to begin to
    measure its performance in order to estimate if defined research objectives had been met.
\end{enumerate}

\section{Overview of the Report}\label{sec:reportOverview}
This report is subdivided into a number of chapters each with a distinct purpose.
The introduction gives an overview of what the project is about, the goals of the project, and how
these goals will be achieved.
The background will present the knowledge that is required in order to proceed to the
later stages of the report with sufficient understanding.
It will also have the purpose of documenting the research that was done throughout the course of the project.
The implementation chapter will discuss how we apply the knowledge that was acquired in the
background to our agent implementation.
This chapter gives a deep dive on the code that was implemented and explains how this code operates
and relates to the abstract algorithms discussed in chapter 2.
The empirical studies chapter then discusses the empirical results gleaned from the work in chapter 3.
The metrics that were used to evaluate our poker agent are discussed, the parameters used for the algorithm
in each experiment are outlined and analysis of the results is given.
Finally, the conclusions chapter gives a retrospective look at the project.
With an overview of how the project met its goals, and a discussion of how the project could be
expanded in the future to add further value.

\section{Contribution}\label{sec:contribution}
The research done throughout this report should contribute to the academic area of reinforcement
learning applied to poker in various ways.
Firstly, we consolidate summaries of a number of approaches that can be taken to solve this problem into a single
resource.
Secondly, a more in-depth description of Heinrich's 2017 application of MCTS to Leduc Hold'em is
provided through coding snippets and detailed explanation.
A great deal of research was required for this implementation with multiple sources being called
upon in order to piece together our implementation, thus we feel having our code
open source and available to the world may aid future researchers in the same area.

\section{Motivation}\label{sec:Motivation}
For the last number of years, I have played poker recreationally with friends or online.
It became more of an interest of mine as I started to explore the mathematical basis for the game and how
players could use their knowledge, intelligence, and temperament in order to gain an advantage in a game that,
on the face of it, seemed to be largely based on chance.
I spent some of my free time researching different aspects of the game.
This included gaining some basic knowledge like the probability of making drawing hands as well as learning more
technical aspects of the game such as how to calculate the expected value of hands,
or how to narrow down one's opponent's range of possible hands.

Concurrent with the development of this interest I was also becoming more and more interested in the area
of ML .
ML and the development of artificial intelligence is possibly the most glamourized area
of computer science.
However, this is probably for good reason because there is something intrinsically interesting
about machines that can learn to solve a problem on their own, without direct instructions from a human.
The fact that ML has made so many strides in recent years was another cause of interest in
this area of computer science, especially as the practical viability of ML as a means of tackling a wide array
of problems in industry continues to increase.

As a result, the merging of these two interests as the basis for my final year project seemed like an obvious choice.