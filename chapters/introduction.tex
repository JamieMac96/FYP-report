\chapter{Introduction}
\label{ch:intro}

\section{Overview}\label{sec:overview}
Since the inception of machine learning(ML), games have been a key problem area that has seen a lot of focus
in academia.
For decades games have been used as a platform to test and develop algorithms that have gone on to provide invaluable
services that are used in peoples everyday lives.
For example\citep{tesauro1995td} demonstrated the value of the reinforcement learning techniques
with his backgammon application.
Today we see these techniques starting to permeate complex industry domains such as vehicle
automation\citep{desjardins2011cooperative}.
The ability to contribute to this great history was a large motivator when it came to choosing this project.

Although this report will, to an extent, discuss ML and how it applies to games in general, the
primary focus will be on ML techniques when applied to texas hold'em, or variations of it.
For the past number of years, a method called Counterfactual Regret Minimization (CFR) has dominated the
top spots in the Annual Computer Poker Competition(ACPC).
CFR is an algorithm that allows computation of a strategy through self-play.
When applied to large imperfect information games such as texas hold'em CFR generally relies on the
creation of abstractions of the game.
These abstractions are used to reduce the size of the state space of the game.
CFR is then applied to these abstractions in order to learn a strategy for the game.
It is thus critically important that these abstractions accurately represent the full game's mechanics.
This requires a high degree of knowledge of the game in order to create an accurate abstraction.

There have also been attempts to tackle texas hold'em using a ML paradigm called reinforcement
learning (RL).
The great appeal of these approaches is that they do not require us to generate custom abstractions of the game.
As a result of this, it was decided that this project would utilise RL techniques to tackle the problem.

RL is a way of programming agents by reward and punishment without needing to specify how the
task is to be achieved\citep{kaelbling1996reinforcement}.
On a surface level RL seems like a perfectly reasonable methodology for solving games such as texas hold'em.
However, texas hold'em is an imperfect information game.
This means that we do not know the entirety of the state information at any given time
i.e we do not know the values of the opponent's cards.
Thus, from a RL perspective, we do not know the actual state from which we are choosing actions.
This makes pure reinforcement learning strategies impractical for these types of games.

However, there has been some success when more custom reinforcement learning methods have been implemented.
In one case linear programming techniques and RL were used in order to tackle a
simplified version of the game\citep{dahl2001reinforcement}.
In another case RL was combined with supervised learning(SL) and game theoretic
techniques\citep{heinrich2016deep} to develop an algorithm called Neural Fictitious Self-Play(NFSP).
This method became the focus of the project for a period of time before a decision was made
that the broad range of techniques involved would add too much risk to the project.
When it was discovered that the same researcher, Johannes Heinrich had applied an RL related search
algorithm called Monte Carlo Tree Search (MCTS) to a number of variations of poker\citep{heinrich2015smooth}
this algorithm became the primary focus of the project.
This algorithm was of great interest for a number of reasons.
Firstly, this relatively new technique had been the basis for the tremendous improvement
in performance of computer Go programs over the last decade.
The culmination of which was the AlphaGo project that defeated the highest ranked human Go player
in the world\citep{silver2016mastering}.
Secondly, this approach had shown the best results of any of Heinrich's documented approaches
with the developed agent placing second in the 2014 ACPC .
Finally, the lead researcher of the AlphaGo project, David Silver had collaborated on Heinrich's
research thus adding significant weight to the research.


\section{Objectives}\label{sec:objectives}
\subsection{Primary Objectives}\label{subsec:primaryObjectives}

Although this project will be largely research based, the primary goal is to create a texas hold'em
playing agent.
Due to the fact that texas hold'em has an extremely large state space, combined with the fact that it
is an imperfect information game, the initial goal will be to tackle a simplified version of the game.
Specifically Leduc Hold'em will be used for this version of the project.
This version of hold'em consists usually of a six card deck and only one private card, compared to two in texas hold'em.

In\citep{heinrich2015smooth} a metric called exploitability was used.
This is a measure of how the agent's strategy fares against the best responses to that strategy.
In other words, if the opponent knows our strategy, and can take the best possible action
in every state in order to maximise their potential gain in reward, exploitability is the
average amount that they would gain from doing so.
For Leduc Hold'em, with a 6 card deck and 500,000,000 iterations the initial exploitability was 2 and converged
to .0223.
As such the success criteria for our initial iteration of the game is to replicate these results, with an
allowance for hardware differences that may impact computational speed.

It is also my goal to create a product that will be fun and useful for the general public.
As such another objective will be to create an application that will allow users to play heads-up against the final product.

\subsection{Secondary Objectives}\label{subsec:secondaryObjectives}
As this project is very specific and academic, one of the larger challenges will be to gain a strong knowledge
of the domain.
This means learning the history of RL, the types of problems that it has been used to solve and the specific details of
different RL algorithms.

A successful project will require a high degree of knowledge from the broader domain of RL. However, it is also the case
that I must become closely familiar with the existing academic literature in the area of RL with respect to imperfect
information games.
This will allow me to avoid taking approaches that have previously shown to fail as well as allow me to add value to
the existing literature whether that be through literature review or through my own experimental findings.


\section{Methodology}\label{sec:methodology}

\textbf{Identify Research Objectives}:

\textbf{Approximation of Systematic Literature Review}:

\textbf{Design and Implement Prototype}:

\textbf{Empirical Studies}:

\section{Overview of the Report}\label{sec:reportOverview}


\section{Contribution}\label{sec:contribution}
The research done throughout this report should contribute to the academic area of reinforcement
learning applied to poker in various ways.
Firstly, we consolidate summaries of a number of approaches that can be taken to solve this problem into a single
resource.
Secondly, a more in-depth description of Heinrich's 2017 application of MCTS to Leduc Hold'em is
provided through coding snippets and detailed explanation.
A great deal of research was required for this implementation with multiple sources being called
upon in order to piece together our implementation, thus we feel having our code
open source and available to the world may aid future researchers in the same area.

\section{Motivation}\label{sec:Motivation}
For the last number of years I have played poker recreationally with friends or online.
It became more of an interest of mine as I started to explore the mathematical basis for the game and how
players could use their knowledge, intelligence and temperament in order to gain an advantage in a game that,
on the face of it, seemed to be largely based on chance.
I spent some of my free time researching different aspects of the game.
This included gaining some basic knowledge like the probability of making drawing hands as well as learning more
technical aspects of the game such as how to calculate expected value of hands,
or how to narrow down one's opponent's range of possible hands.

Concurrent with the development of this interest I was also becoming more and more interested in the area
of ML.
ML and the development of artificial intelligence is possibly the most glamourized area
of computer science.
However, this is probably for good reason because there is something intrinsically interesting
about machines that can learn to solve a problem on their own, without direct instructions from a human.
The fact that ML has made so many strides in recent years was another cause of interest in
this area of computer science, especially as the practical viability of ML as means of tackling a wide array
of problems in industry continues to increase.

As a result the merging of these two interests as the basis for my final year project seemed like an obvious choice.