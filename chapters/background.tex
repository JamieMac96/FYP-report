\chapter{Background}
\label{ch:background}

The aim of this chapter is to give the reader background information on certain areas of machine learning and
game theory in order for them to understand the rest of the report.
There will also be a in-depth discussion of existing literature that relates to machine learning in texas hold'em
agents.

Machine learning is an area of computer science that tackles how we construct computer programs that improve
with experience\citep{mitchell1997machine}.
The term was coined by Arthur Samuel in a paper in which he discussed machine learning methods using
checkers\citep{samuel1959some}.
Since then there has been a great deal of advancement in the field.
Some of the notable early contributions being the discovery of recurrent neural networks in 1982,
the advancement of reinforcement learning by the introduction of Q-Learning in 1989 and the
development of a backgammon-playing agent using neural networks and temporal difference learning\citep{tesauro1995td}.
Recently we have seen some of this early academic work culminate in more practical achievements such as
Facebook's DeepFace system which, in 2014,  was shown to be able to recognise faces at a rate of 97.35\% accuracy,
a rate that is comparable to that of humans.
Another example of recent achievement is Google's AlphaGo program which, in 2016, became the first program to beat
a professional human player.

It should be becoming clear that machine learning can be a solution to a wide array of problems and as
both hardware and software continue to improve it's reach will only continue to grow.
We are starting to see machine learning systems become a key component of many companies business model.
Since certain machine learning techniques are great at prediction, machine learning has been widely used
for content discovery by companies such as Google and Pinterest.
Other business applications include the use of chatbots as a part of customer service, self-driving cars
and even in the field of medical diagnostics.


\section{Reinforcement Learning}\label{sec:reinforcementLearning}
The early research for this project yielded reinforcement learning as the most suitable machine learning
paradigm for the problem of texas hold'em.
However, in order to understand both reinforcement learning and how it would apply to the chosen problem,
in depth research was required.
This research included a Udemy course\citep{udemy2018rlpython} as well as reading in part the book
Reinforcement Learning: An Introduction\citep{sutton1998reinforcement}.

\begin{figure}[ht]
    \includegraphics[scale=.5]{RL_illustration.png}
    \caption{Reinforcement Learning}
\end{figure}

As mentioned in the introduction, reinforcement learning is a method of programming agents by reward and
punishment without needing to specify how the task is to be achieved.
As such the primary components of a reinforcement learning problem are an agent which exists in an environment.
From a simplified perspective we can think of the environment as a set of states, actions and rewards.
The objective for the agent is to maximise cumulative reward.
This is done by developing a policy that will dictate which actions should be taken in each state.

\subsection{Explore-Exploit Dilemma}\label{subsec:exploreExploit}
When it comes to reinforcement learning one of the first questions that we have to ask is how we explore
the state space.
An example that is often used to conceptualize this problem is the multi armed bandit problem.
Let's say an agent is in a room with a number of gambling machines.
Each of these machines has an arm that, when pulled will return a reward of 0 or 1 based on some underlying
probability\citep{kaelbling1996reinforcement}.
The agent has a limited number of total pulls.
So the question becomes how do we distributed these pulls in order to maximise return?
Well, first we have to ensure that we explore enough that we find the machine with the best reward probability
and second, we must then exploit this machine to the best of our abilities.

\begin{figure}[ht]
    \includegraphics[scale=.25]{bandit.png}
    \caption{Multi Armed Bandit}
\end{figure}

There are a number of approaches that can be taken to solve this problem, we will now briefly discuss two of these
methods.

\subsubsection{$\epsilon$-Greedy Solutions}\label{subsec:eGreedy}
The first approach that we will discuss is the $\epsilon$-greedy strategy.
This approach was first proposed in\citep{watkins1989learning} and is a very simple and widely used method.
The $\epsilon$-greedy strategy involves choosing a random lever some proportion $\epsilon$ of the time, and
choosing the lever that has been established to give the highest reward the rest of the time.

There are a number of variations of this method, the first being the $\epsilon$-first strategy.
With this strategy we take all of our random choices first, allowing us to establish the best bandit,
after which we exploit this bandit.
However, as stated in\citep{vermorel2005multi} this simple approach is sub-optimal because asymptotically,
the constant factor prevents the strategy from getting arbitrarily close to
the optimal lever.

This is where the $\epsilon$-decreasing strategy becomes useful.
Here, the proportion of random lever pulls decreases with time.
Generally if our initial epsilon value is $\epsilon_0$ then our epsilon value at time $t$ will be
$\epsilon_t = \frac{\epsilon_0}{t}$.

\subsubsection{Interval Estimation Strategy}
Another approach that can be used is called the interval estimation strategy.
With this method we initially give an optimistic estimate of the reward to each bandit within a certain
confidence interval.
Then we simply take a greedy approach to our exploration.
Less explored bandits will have a artificially higher reward estimate and thus they will be greedily chosen,
thus allowing us to evaluate each of the bandits.

In the context of reinforcement learning, state space exploration through the $\epsilon$-greedy approach is
generally sufficient.

\subsection{Markov Decision Processes}\label{subsec:mdp}
In the last section we have established some methods that can be used to explore environments.
We will now discuss in more detail how reinforcement learning environments, and their interaction
with reinforcement learning agents, are modelled.
Generally finite Markov decision processes (finite MDPs) are used.
Markov decision processes provide a formal mathematical framework for sequential decision making,
where actions influence immediate rewards as well as subsequent situations\citep{sutton1998reinforcement}.
MDPs allow us to create an idealized model of reinforcement learning problems and thus we can make precise
theoretical statements.

\subsubsection{MDP Dynamics}

As mentioned earlier, reinforcement learning problems consist of an agent and an environment interacting.
Markov decision processes can be looked at in a similar way.
However, there are a number of additional factors that we must consider in order to paint a complete picture.

We can think of the problem as consisting of a set of discrete time steps.
At each time step the environment supplies the agent some information about the state, $S_t$.
Using this information the agent chooses an action, $A_t$.
Then, as a result of the action, the environment will supply the agent with a reward, $R_t$, as well as a new state.
As such the process of interaction between the agent and the environment can be seen as a trajectory of states,
actions and rewards\citep{sutton1998reinforcement}:
\begin{align}
    S_0,A_0,R_1,S_1,A_1,R_2,S_2\dots
\end{align}

Thus far we understand that states, actions and rewards are related, however questions still exist as to
the exact workings of this relationship.
The answer is that finite MDPs contain a discrete probability distribution that determines the likelihood that we
will reach the state $s'$ and receive reward $r$ at time $t$ based on the previous action $a$ and state $s$:
\begin{align}
    p(s',r|s,a) \doteq \Pr\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\}
\end{align}
In simplified terms this means that for a given state-action pair $(s,a)$, the probability of us reaching some
new state $s'$ and receiving reward $r$ is determined by the MDPs probability function $\Pr$.

This four argument probability function completely characterizes the dynamics of the MDP and from it anything
else we want to know about the environment\citep{sutton1998reinforcement}.

\subsubsection{MDPs and Learning}
The goal of the agent in an MDP is to learn how to maximise the cumulative reward received when traversing
the environment.
In some cases we will traverse the MDP until we reach some terminal state, $T$.
This type of MDP reflects episodic tasks that will always terminate.
In this case we can calculate the cumulative reward,$G_t$ as follows:
\begin{align}
    G_t = R_{t+1}+R_{t+2}+R_{t+3}+\dots+R_T
\end{align}

However, in other cases we will model continuous tasks.
The problem here is that, if we use the same method of calculating $G_t$ as we do for
episodic tasks then in this case $G_t$ will always eventually sum to infinity, regardless
of whether we are taking good or bad actions.
As such we must introduce the a new concept called discounting.
With this approach the aim is to maximise the sum of future discounted rewards.
Thus $\gamma$, a parameter with a value between 0 and 1, is introduced.
As such, for continuous tasks modelled as MDPs our cumulative reward is as follows:
\begin{align}
    G_t = R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\dots
\end{align}

Based on our specified value of $\gamma$ we can alter the weighting of future rewards.
For example if we have a low value for $\gamma$ (eg .5) then the value of rewards more than a
few time steps in the future will be very low.

\subsubsection{Partially Observable Markov Decision Processes}
A partially observable Markov decision Process (POMDP) is used to model environments that are not
fully observable\citep{kaelbling1996reinforcement}.
POMDPs extend MDPs by including a set of observations, $O$.
There is also an observation function, $P(O_t=o|S_t=s)$, that determines the probability of making a certain observation
in a certain state.
Hold'em can be modelled as a POMDP due to the fact that is an imperfect information game.

However, it is possible that an agent in a POMDP environment can remember the sequence of observations
and actions that lead to the current state.
This is a sufficient statistic of it's experience and can thus define a complete information
state\citep{heinrich2017reinforcement}.
As such we can reduce the POMDP to it's underlying MDP by using these full history information
states and also extending the relevant transition and reward functions.


\subsection{Policy Evaluation and Policy Improvement}\label{subsec:policyEvalPolicyImp}
As mentioned above the primary focus of reinforcement learning is to find a policy (denoted by $\pi$) that allows
the agent to take actions in states that lead to the maximum possible reward.
There are two primary problems that we must solve in order to do so.

The first is called the prediction problem, or policy evaluation.
This involves computing the values of states given some arbitrary policy\citep{sutton1998reinforcement}.
For example a state would have a high value if the reward for reaching that state was high.
A state would also have a high value if we were only one action away, according to the supplied policy,
from a state that renders a high reward.
However a state would have a low value if, according to the policy, there was no path to a state that
would return a positive reward in the foreseeable future.

The second problem is known as the control problem, or policy improvement.
This involves changing the policy in order to improve our cumulative reward.
The policy improvement process can only occur when the we have performed policy evaluation.
Let's say, after our evaluation step, we know the value of some state $s$.
Note that this value is calculated with the condition that we take some action $a$ in state $s$.
But, if we take some other action $a'$ would this render a higher value for $s$?
If the answer is yes then we update the policy.

These two operations can be seen as the core of reinforcement learning.
In the next section we will discuss different reinforcement learning methodologies.
Some of the main differences are in how method each approaches the prediction and control problems.

\subsection{Dynamic Programming}\label{subsec:dp}
Dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a
perfect model of the environment as a Markov decision process\citep{sutton1998reinforcement}.
DP is not widely used in practical reinforcement learning applications due to its assumption
of a perfect MDP and it's high computational requirements.
Despite this it is very important from a theoretical standpoint as it serves as an introduction to a number
of important reinforcement learning concepts.
Furthermore, it provides a basis for many algorithms that are used in practical reinforcement learning applications.

\subsubsection{Policy Evaluation in Dynamic Programming}
When discussion policy evaluation we talk about a state-value function or a value function.
This is simply the mapping of states to their corresponding values and is denoted by $v$.

Since the environments's dynamics are completely known we can apply an iterative solution to finding
the value function.
If we consider a series of approximate value functions $v_0, v_1, v_2,..$.
The initial value function, $v_0$ is chosen arbitrarily and each successive generation is obtained by
using the Bellman equation for $v_\pi$ as an update rule\citep{sutton1998reinforcement}:

\begin{align}
    v_{k+1}(s) &= \EX_{\pi}\lbrack R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s \rbrack
\end{align}

In order to produce each successive approximation of $v_{k+1}$ from $v_k$ we apply the operation outlined to each
state $s$.
As shown above our new value for $s$ is based on a combination of the expected immediate rewards($R_{t+1}$),
and the expected values of each of the states that we can transition to($S_{t+1}$) given policy $\pi$.
It can be shown that as $k\rightarrow\infty$ $v_k$ will converge to $v_\pi$, the correct value function for policy
$\pi$.
This algorithm is called \textit{iterative policy evaluation}\citep{sutton1998reinforcement}.

\subsubsection{Policy Improvement in Dynamic Programming}
Since we have now determined how good it is to follow $v_\pi$ we can use this information to determine how
we should modify this policy in order to improve it's value.
If we assume that $\pi$ is a deterministic policy then $\pi(s)$ will return some action that we must take.
Now the question becomes what if we take some other action $a \neq \pi(s)$?
Well we must consider whether or not choosing this action, and then continuing to use the existing policy will
improve the value of the policy.
If it does then we will choose this new action.

The logical extension of this approach is to apply it to each state and each possible action.
As such we will select what appears to be the best action at each state.
We can thus denote our new greedy policy $\pi'$ as:
\begin{align}
    \pi'(s) = argmax\EX_{\pi}\lbrack R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a\rbrack
\end{align}
Essentially here we are determining the value of each available action in the current state, using the same operation
as outlined in the policy evaluation phase.
Then the argmax function will select the action with the highest value.
Finally we assign this action to be the one we will choose in state s, according to the new policy $\pi'$.

Note that in this section we have outlined an algorithm with respect to deterministic policies,
however a lot of times in reinforcement learning we deal with stochastic policies.
This means that we take actions in states according to some probability distribution rather than always choosing
the same action in a particular state.
This is not a problem as the ideas mentioned apply equally well to stochastic policies.

\subsection{Monte Carlo}\label{subsec:mc}
Monte Carlo(MC) methods are a wide range of algorithms that rely on random sampling in order to obtain results.
We could apply such a method in the case of having an array of 1 trillion elements that could be
either 1 or 0.
If we wished to calculate the number of 1s in such an array through iterating the array it would
be very computationally costly.
However, we could randomly generate indices and maintain a count of both the number of 1s found at these indices
and the number of indices generated.
Based on these figures it would be possible to estimate the total number of 1s in the array.
Over time as we sampled more and more our estimate would converge towards the true value.

Monte Carlo based reinforcement learning techniques apply this method in the policy evaluation step.
In Monte Carlo, unlike dynamic programming, we do not assume complete knowledge of the environment.
Monte Carlo methods require only experience.
We sample sequences of states, actions, and rewards from interaction with the
environment\citep{sutton1998reinforcement}.
These sequences are called episodes.
Monte carlo evaluation is an episodic process this means that we only update our action values after an episode
has completed.

\subsubsection{Monte Carlo Policy Evaluation}
In Monte Carlo methods we take a fundamentally different approach to policy evaluation.
As mentioned this method is focused on using episodic experience.
In order to evaluate a state we can simply average the rewards returned after visiting that state.
As we observe more returns the average will converge to the actual expected value of the state.

It is worth noting that a state $s$ could be visited more than once in an episode.
As such we can either average the returns following the first visit to $s$ or we could average the returns
after each visit to $s$.
These two methods are called \textit{first-visit} and \textit{every-visit} respectively.

\subsubsection{Monte Carlo Action Values}
In Monte Carlo methods, the lack of a model means that we cannot use only state values in order to
obtain a policy.
Rather state-action pairs are generally evaluated.
This mapping of state-action pairs to values is referred to as the $q$ function or the action value function.
In this case the evaluation problem for action values is to estimate $q_\pi(s, a)$, the expected return when
starting in state s, taking action a, and thereafter following policy $\pi$\citep{sutton1998reinforcement}.

The method for policy evaluation using state-action pairs is almost identical to that outlined above.
The only difference being that instead of averaging rewards for each state, we average
rewards for each action taken when a state is visited.
There is one problem with this approach in the context of deterministic policies.
The problem being that in following a deterministic policy we will only receive returns
for a single action, thus only one action value estimate will be improved.
In order to negate this problem we can specify that every episode begin in a state-action pair,
with the probability of starting in each state-action pair being non-zero.
This is called the \textit{exploring-starts} method.
Another approach would be to ensure that we are using a stochastic policy with
the probability of selecting each action being non-zero.

\subsubsection{Monte Carlo Policy Improvement}
In Monte Carlo methods, the overall policy improvement algorithm is the same as outlined in the dynamic
programming section.
That is, we alternate between modifying the value function to more closely approximate the current policy,
and using the value function to improve the policy.
\begin{figure}[ht]
    \includegraphics[scale=.5]{MC_control.png}
    \caption{Monte Carlo Policy Improvement}
\end{figure}

\subsection{Temporal Difference Learning}\label{subsec:td}
The final reinforcement learning method we will discuss is the Temporal Difference(TD) learning method.
TD learning combines dynamic programming(DP) and and Monte Carlo(MC) ideas\citep{sutton1998reinforcement}.
Like MC, we can learn directly from experience, without a model of the environment's dynamics.
However, like DP we update state value estimates based on other learned estimates, without needing to wait for an
episode to complete and the return of some final outcome.

The selective use of different aspects of these reinforcement learning methodologies by TD learning has
a number of advantages.
Obviously the fact that a model of the environment is not needed makes it easier to implement TD methods
compared to DP .
TD methods are also conducive to solving problems with long episodes or even continuous tasks with no
episodes at all due to the fully online nature of this learning algorithm.

\subsubsection{TD Learning Policy Evaluation}
Unlike MC, with TD learning we need only wait until the next time step in order to update the value function.
This is exemplified by the \textit{TD(0)} or \textit{one-step TD} method in which we make the
update immediately on transition from state $S_t$ to state $S_{t+1}$.
The more general case of this method is the \textit{TD($\lambda$)} or \textit{n-step TD}.
With \textit{TD(0)} the update rule is as follows:
\begin{align}
    V(S_t) \leftarrow V(S_t) + \alpha \lbrack R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \rbrack.
\end{align}
As such the new value for some state $S_t$ is dependant on the previous value of the state($V(S_t)$),
along with the reward($R_{t+1}$) gained from transitioning to state $S_{t+1}$ plus the discounted($\gamma$)
estimated value of that state($V(S_{t+1})$).
The sum of the latter is multiplied by $\alpha$ which is a small positive fraction that influences the
learning rate.

This rule is applied for each state visited in an episode and for each episode.

\subsubsection{TD Learning Policy Improvement - SARSA}
At this point it is worth noting that there are two distinct methods of handling policy improvement.
The first is on-policy and the second is off-policy.
On-policy reinforcement learning is when the policy being evaluated or improved is the same policy that is used
to make decisions.
In off-policy reinforcement learning the policy being used to generate behavior is not the same as the policy
being evaluated or improved.

SARSA is an example of an on-policy algorithm.
The policy improvement mechanism is the same here as outlined in the previous sections.
With SARSA, like in Monte Carlo we utilise the action-value function($q_{\pi}(s, a)$) rather
than the state-value function.
Thus the policy evaluation step is slightly modified as follows:
\begin{align}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \lbrack R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \rbrack.
\end{align}
This update utilises the quintuple of events, ($S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$), which is where the
name SARSA originates.


\subsubsection{TD Learning Policy Improvement - Q-Learning}
Q-Learning is an off-policy control algorithm.
This was one of the early breakthroughs in reinforcement learning as it allows the direct approximation of
the optimal action value function independent of the policy being followed.
The update rule is as follows:
\begin{align}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \lbrack R_{t+1} + \gamma maxQ(S_{t+1}, a) - Q(S_t, A_t) \rbrack.
\end{align}
Here our update rule is similar to that of SARSA apart from the fact that we use the value of the best
action available ($maxQ(S_{t+1}, a)$) when updating our action value.

\subsection{Monte Carlo Tree Search}\label{subsec:mcts}
In this section we will discuss Monte Carlo Tree Search(MCTS), a decision time planning algorithm.
This algorithm uses tree search combined with value estimations from MC simulations in order to
efficiently solve very large MDPs.
One application of this algorithm was in DeepMind's AlphaGo\citep{silver2016mastering} which became the
first computer program to beat a highly ranked Go champion.

MCTS is an episodic process and there are four key steps involved in each of these episodes.
The first step is \textbf{selection}.
We begin at the root of the tree and use the tree policy, based on action values associated with the edges
of the tree, in order to select a child node at each level\citep{sutton1998reinforcement}.
The second step is \textbf{expansion}.
When we reach a leaf node in the tree we may add one or more child nodes based on
the actions available at that point.
The third step is \textbf{simulation}
When we have exited the tree we then switch to using the rollout policy (eg a random policy) in
order to reach a terminal node in the tree from which we can obtain a reward.
The final step is \textbf{backup}.
This step is uses the reward obtained to update the values of the tree nodes that were visited
during the episode.

In summary this algorithm uses the simulation step in order to efficiently sample rewards which
are then propagated back to nodes of the tree.
In later iterations and based on these values the tree policy selects which areas of the tree to
explore and expand.

\section{Game Theory}\label{sec:gameTheory}
After taking a deep dive on reinforcement learning and the papers surrounding RL in texas hold'em it became
clear that a pure reinforcement learning approach would not be feasible.
The main reason for this was the fact that texas hold'em is an imperfect information game.
As outlined by\citep{dahl2001reinforcement}:
\begin{quotation}
Note that the concept of game state values, which is the key to solving perfect information games,
does not apply to imperfect information games, because the players do not know from  which game states they
are choosing.
\end{quotation}
As such it became apparent that some other techniques would have to be incorporated in order to create a competent
texas hold'em agent.
Since our implementation will be following the method discussed by\citep{heinrich2017reinforcement}
the following section will give the requisite background in order to facilitate later discussion of this method.

\subsection{Modelling Games}\label{subsec:modellingGames}
When we attempt to solve a game, we must first understand how we are to model that game.
We must also carefully consider how we define a form or structure of the model\citep{myerson2013game}.
In this section we will discuss three important forms that games can take and the varying utility
of each in different scenarios.

\subsubsection{Extensive Form Games}
Extensive form games are a model of players sequential interaction, with explicit representation of a
number of key aspects of the game.
They were formally defined in\citep{kuhn2016extensive} as consisting of the following components:
\begin{itemize}
    \item \textit{N} - a set of players.
    \item \textit{S} - a set of states that represent the nodes of a rooted game tree.
    \item \textit{A(s)} - a set of actions for each state representing the edges to the following states.
    \item \textit{U} - a set of information states (one set for each player).
    \item \textit{Player Function} - determines who is to act at a given state.
    \item \textit{Information Function} - determines which states are indistinguishable for the player by mapping them to the same information state.
    \item \textit{Return Function} - maps terminal states to each player's return/payoff.
\end{itemize}

Extensive form games allow us to richly describe game situations.
This allows us to uncover characteristic differences between games and the structural features which
determine these differences\citep{kuhn2016extensive}.

We can also define behavioral strategies for players, which consist of a probability distribution over actions
given information states\citep{heinrich2017reinforcement}.
This is denoted by $\pi^i(a|u^i)$.
If we have a collection of strategies for all players in the game then this is called a strategy profile, $\pi$.
$\pi^{-i}$ is the set of strategies in $\pi$ not including $\pi^i$.
This will be discussed later when we discuss Heinrich's implementation in more depth.

\subsubsection{Normal Form Games}
Normal form games are represented by way of a matrix.
Although some information is lost in comparison to extensive form games, normal forms are
better suited to the derivation of generalized theorems\citep{kuhn2016extensive} and thus have their own utility.

An extensive-form game can induce an equivalent normal form of the same game.
This can be done through the generation of a set of deterministic behavioral strategies for each player
called pure strategies.
Each pure strategy is a full game plan that will determine an action for each situation the player may encounter.
We can also create mixed strategies which define a probability distribution over the players pure strategies.
We can denote a mixed strategy for player $i$ as $\Pi^i$.
When we restrict the extensive-form return function to normal-form we yield an expected return function.
The expected return function for some mixed strategy profile $\Pi$ is $R^i(\Pi)$\citep{heinrich2017reinforcement}.

\subsubsection{Sequence Form Games}
In order to compute a Nash equilibrium for an extensive form game we can convert the game to a normal form,
however normal forms tend to produce very large game trees.
The sequence form is an efficient method of representing extensive form games\citep{koller1996efficient}.
This representation is described as a linear-sized strategic description of the game.
It decomposes players strategies into sequences of actions and probabilities of realizing those sequences.

In sequence form games, for every player $i\in N$, each of their information states $u^i\in U^i$
uniquely define a \textbf{sequence} $\sigma_{u^i}$ of actions that a player must take in order to reach
that information state.
These sequences are then mapped to realization probabilities through what is called a realization plan,
denoted by $x$.
When two or more strategies have the same realization plan we consider these strategies to be
realization equivalent\citep{von1996efficient}.
This can apply across different types of strategies for example extensive-form, behavioral strategies
and normal-form, mixed strategies\citep{kuhn2016extensive}.

\subsection{Nash Equilibria}\label{subsec:nashEquilibria}
A Nash's equilibrium is a state in which each player in a game has chosen a strategy and
none of the players can benefit from changing their strategies, if the other player's strategies remain unchanged.
As such, if we reach a strategy that induces a Nash's equilibrium our strategy can no longer be exploited.

In the context of extensive form games the concept of best responses is related to that of Nash's equilibria.
If the opponent's strategies are denoted by $\pi^{-i}$ then the set of best responses are denoted by
$BR^i(\pi^{-i})$.
Note that if we have a strategy profile $\pi$ such that $\pi^i\in BR^i(\pi^{-i})$ for every $i\in N$ (i.e
for every player) then that game constitutes a Nash equilibrium\citep{heinrich2017reinforcement}.
Heinrich also discusses the concept of $\varepsilon$-best responses.
This is the set of strategies that are within a certain tolerance $\varepsilon$ of the best responses.
As such we can define an $\varepsilon$-Nash equilibrium as a strategy profile $\pi$ such that
$\pi^i \in BR^i_\varepsilon (\pi^{-i})$ for all $i\in N$.

\subsection{Fictitious Play}\label{subsec:fictitiousPlay}
Fictitious play(FP) is a game-theoretic model of learning through self-play.
At each iteration players choose the best responses to their opponents average
strategies\citep{heinrich2017reinforcement}.
These strategies converge to Nash equilibria in certain classes of games, including two-player, zero sum games.

Generalised weakened fictitious self-play (GWFSP) is a method that is built on FP but allows for approximations
in players strategies\citep{leslie2006generalised}.
Thus it is more suitable for machine learning.
GWFSP allows for a certain error at each iteration of the algorithm and relies on the fact that this
error rate will tend towards zero as time progresses.
In the the research done by Leslie and Collins normal form games were studied.
There has also been research done into the applicability of FP to extensive form games however,
before Heinrich there was no method shown to converge for imperfect information games such as poker.
In later sections we will discuss how Heinrich utilised GWFSP as a basis for neural fictitious self-ply,
a method that has shown success in games of imperfect information.

\section{Supervised Learning}\label{sec:supervisedLearning}
Supervised learning was also a common theme in the RL based texas hold'em implementations so once again a
brief study of this area was conducted.

Supervised learning involves an agent which observes some example input-output pairs and learns
a function that maps from input to output.\citep{russell2016artificial}.
This learned function can then be used on new input data, that wasn't used to train the agent and the
agent should be able to give an accurate output.
The agent must be able to identify general features of the input data and how they map to the output.
Common applications of supervised learning include computer vision, speech and pattern recognition as well as
spam detection.

\section{Texas Hold'em}\label{sec:thIntro}
Texas hold'em is one variant in a family of games called poker.
Poker is a group of card games that combine gambling, strategy and skill.
All poker variants have three core similarities.
There is betting involved, there is imperfect information (ie cards remain hidden until the end of a hand)
and the winner is determined by combinations of cards.

\subsection{Game Structure}\label{subsec:bettingRounds}
Texas hold'em consists of four betting rounds.
Initially each player is dealt two private cards.
These remain face down and only the person who received these cards may view them.
In the next three rounds five public cards are dealt face up on the table.
The second round of dealing is called the flop, where three public cards are dealt.
The third round is called the turn where one additional public card is dealt.
Finally in the fourth round another public card is dealt which is called the river.

At each round, after the cards are dealt, the players are given the opportunity to take a number of betting
related actions.
We will discuss the permitted actions in the next section.

In order for players to be incentivized to continue playing in a wider array of situations, blinds are required.
Blinds are a mandatory bet that must be posted by two of the players present at the game.
These two bets are called the big blind and the small blind, the big blind being twice that of the small blind.
As hands are played the big and small blinds are posted by different players in order to distribute the cost fairly.

The big and small blind are the first two bets that contribute to what's called the pot.
The pot is the collection of all of the current chips bet by the players.
When a player wins a hand then what they receive in return is the pot.

The final structural component of the game is player stacks.
Each player will start the game with a certain amount of chips.
If a player wins a pot then all of the chips in the pot are transferred to the winners stack.

\subsection{Actions}\label{subsec:actions}
As mentioned in the previous section, after cards are dealt players are permitted to take a number of actions.
If a player is the first to act the they may either check or bet a chosen amount.
If a player is not first to act and the previous player has made a bet then they may choose
to either fold and forfeit the pot, call the bet by adding the same amount to the pot, or raise by
adding the amount previously bet plus some additional chips.
Players can go back and forth with bets until they run out of chips in which case they are
considered to be "all in".

\subsection{Hand Values}\label{subsec:handValues}
In poker the best 5 cards available to the player can be played.
This means any combination of his own private cards and the public cards can be used.
There are 10 major poker hands.
These are listed below in ascending order of value:
\begin{enumerate}
    \item \textbf{High Card:} None of the higher hand values achieved, highest card plays.
    \item \textbf{Pair:} Any two cards of the same rank.
    \item \textbf{Two Pair:} Two different pairs.
    \item \textbf{Three of a kind:} Three cards of the same rank.
    \item \textbf{Straight:} Five cards in a sequence.
    \item \textbf{Flush:} Five cards of the same suit.
    \item \textbf{Full House:} Three of a kind with a pair.
    \item \textbf{Straight Flush:} Five cards in sequence, all of the same suit.
    \item \textbf{Royal Flush:} A, K, Q, J, 10 - all of the same suit.
\end{enumerate}

\subsection{Leduc Hold'em}\label{subsec:leducHoldem}
Leduc Hold'em is a simplified version of Hold'em that was first introduced in\citep{southey2012bayes}.
In Leduc Hold'em the deck is reduced to six cards with two suits and three ranks in each suit.
Rather than four rounds there are only two.
In the first round a single private card is dealt to each player.
In the second round a single board card is revealed.
In the first round both players have a mandatory bet of one and a raise of two is allowed.
In the second round players can raise by four.
Each round allows for at most two bets.


\section{Variations of MCTS applied to poker}\label{mctsPoker}
As mentioned MCTS has been shown to be a very powerful method for solving large perfect-information
games such as Go.
However if we wish to apply this algorithm to imperfect information games like poker then 
a number of modifications must be applied.
Such an approach was outlined in\citep{heinrich2017reinforcement} in which a variation of
MCTS called smooth UCT was implemented to tackle both leduc hold'em and limit hold'em.

One of the subtleties of poker is that player information is asymmetric.
In other words each player has access to their own private card 
information but not to their opponents private cards.
This means that we cannot represent the search tree as a single, collective 
entity\citep{heinrich2017reinforcement}, rather two search trees must be available 
to accommodate self-play.
The method used by Heinrich to accomplish this goal was the creation of 
an information function $I^i(s)$.
This function will return the information state $u^i$ of the current player ($i$) given
the current state $s$ from the overall game tree.
Note that the overall game tree will have separate nodes for any variation 
in either players cards.
However, player 1's game tree will not have separate nodes where the only 
differentiating factor is player 2's private cards\citep{johanson2011accelerating} 
due to the fact that this information is not available.
This means that a number of nodes that are separate in the overall game tree will 
be grouped in either players individual game trees.

In figure 2.4 we have shown Heinrich's algorithm for extensive form MCTS .

\begin{figure}[!ht]
    \includegraphics[scale=.6]{images/extensive_form_mcts.png}
    \caption{Extensive Form MCTS - Johannes Heinrich 2017 PhD Thesis}
\end{figure}

In order to understand smooth UCT we must first explain the meaning of UCT .
The abbreviation UCT refers to upper confidence bound(UCB) applied to trees.
This means that in the action selection portion of the algorithm a UCB approach 
is taken, where less explored nodes in the tree are given a positive bias in value.
This means that unexplored nodes will be explored which ensures that we discover 
the best actions at each position in the tree.
The value of an action is denoted as follows:

\begin{align}
Q(u^i, a) + c \sqrt{\log N(u^i) / N(u^i, a)} 
\end{align}

In this expression the $Q$ function denotes the current value estimates for each action $a$ 
in information state $u^i$.
$N(u^i)$ and $N(u^i, a)$ denote the current visitation count of the information state $u^i$
and the subsequent information state after action $a$ has been taken.

This extension of the extensive form MCTS algorithm is shown in figure 2.5.

\begin{figure}[!ht]
    \includegraphics[scale=.6]{images/uct.png}
    \caption{UCT - Johannes Heinrich 2017 PhD Thesis}
\end{figure}

Smooth UCT is a modification of UCT that is inspired by fictitious play\citep{heinrich2017reinforcement}.
As described in the last section the action selection in UCT is purely deterministic.
Smooth UCT changes this by utilizing the average strategy a certain proportion of the time.
In order to calculate the average strategy for a particular state we create a 
probability distribution based on the visitation counts of the actions 
available at that state.
For example we could have an information state $u^i$ that has been visited 100 times 
and has three available actions, $a_1, a_2, a_3$.
Then it would be possible that $a_1$ has been visited 50 times, $a_2$ 35 times 
and $a_3$ 15 times.
According to the average strategy we would then select $a_1$ 50\% of the time, 
$a_2$ 35\% of the time and $a_3$ 15\% of the time.
In order to determine when the average strategy should be used compared to the 
UCB approach Heinrich describes a sequence $\eta_k$.
This sequence decays to 0 as $\lim_{k \to \infty}$ and is expressed as follows:

\begin{align}
\eta_k = max(\gamma, (1 + d * \sqrt{N_k})^-1)
\end{align}

Here $N_k$ is the total number of plays and $\gamma$ is a lower limit on $\eta_k$ 
and $d$ is a constant that parameterises the rate of decay.

This final modification of the algorithm is shown in figure 2.6.

\begin{figure}[!ht]
    \includegraphics[scale=.6]{images/smooth_uct.png}
    \caption{Smooth UCT - Johannes Heinrich 2017 PhD Thesis}
\end{figure}
 

\section{Other Approaches}\label{sec:thImplementations}
Although we have already mentioned that we will be focusing on MCTS
it's worth discussing some other notable methods for creating poker 
agents before we continue.
Currently the premier method for tackling full-scale texas hold'em is counterfactual 
regret minimization.
This is the method that has dominated the Annual Computer Poker Competition(ACPC) for the 
last number of years, however recently some new methods have been outlined 
which look promising and thus we will discuss these methods as well.


\subsection{Counterfactual Regret Minimization}\label{subsec:thCFR}
Counterfactual regret minimization(CFR) is a method for finding approximate Nash
equilibria in imperfect information games and was first outlined in\citep{zinkevich2008regret}.
Regret is a measure of the difference in utility between following some strategy 
$\sigma$ compared to another strategy.
Zinkevich introduced counterfactual regret which is regret applied to a 
single information set ie a single extensive form game state.
It was found that by calculating and minimizing regret on an individual
state basis that there were performance benefits as well as an improvement 
in accuracy of the calculated approximate Nash equilibria.

One implementation of CFR is outlined in\citep{jackson2013slumbot}.
This paper outlines the implementation used for Slumbot, the 2018 ACPC champion 
in no-limit hold'em.
Jackson describes his use of CFR in order to generate a static strategy that approximates
a Nash's equilibrium.
At each iteration a strategy would be computed and when the process had ended an 
average of these strategies would be used.
Jackson also mentions using an abstraction of the game in order to reduce no-limit's 
massive state space (roughly $10^{164}$ with stack sizes of 100 big blinds)\citep{johanson2013measuring}.
This is done through techniques like grouping similar hand values into buckets and 
treating them as strategically equal or splitting the game up into it's individual rounds 
and solving the rounds separately.
These techniques allow for a dramatic reduction in state space and if handled correctly, 
should allow for a strategy that can transfer to the full version of the game successfully.


\subsection{Neural Fictitious Self-Play}\label{subsec:thNFSP}

