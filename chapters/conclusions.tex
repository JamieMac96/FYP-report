\chapter{Conclusions}\label{ch:conclusions}
In this chapter there will be a review of the work done throughout the course of this project.
A summary of the findings will be given along with a reflection on how the project was conducted.
Finally, a brief discussion on future work will be given.

\section{Summary}\label{sec:summary}
The initial goal of this final year project was to explore the possibility of applying
reinforcement learning techniques to imperfect information games.
The initial focus of the project was on the game of Texas hold'em with research being
done into different RL methods that could be used in order to craeate an agent for the game.
It was decided that Monte Carlo Tree Search would be the method used and there would be
an attempt to recreate the results demonstrated in\citep{heinrich2017reinforcement}.

When it came to creating a prototype implementation the poker variation used was a
scaled-down version of Texas hold'em called Leduc Hold'em.
A number of experiments were conducted in which a variety of training methods were used
for the agent.
From the first to last experiment we saw a significant improvement in performance of the
agent with the results trending in the same manner as Heinrich had reported.
Although our results trended as expected we did not achieve the same numeric values as Heinrich,
with our agent's exploitability(2.2) being higher than his (.5) after 1,000,000
iterations of the smooth UCT algorithm.

Despite the fact that exact replication of Heinrich's results was not achieved we can
think of the improving trend of our results to be a success.
This is especially true when we consider the fact that Heinrich did not provide
an algorithm for exploitability calculation and thus we cannot be sure that our metrics
were exactly the same.

When the Smooth UCT algorithm had been implemented, a prototype game was created that
allowed users to interact with the trained agent.

\section{Reflections}\label{sec:reflections}
Although the goals set out for the project were not fully achieved, the project can
still be thought of as a success.
We provided the first known open source implementation of Heinrich's smooth UCT algorithm and
documented the implementation process, making it easier for future researches implement and
utilise this algorithm.
Due to the fact that this algorithm has been shown to be applicable to limit Texas hold'em\citep{heinrich2015smooth}
we see this as a significant achievement.

If I were to attempt this project again with my current knowledge more time would be spent on the
implementation process of the algorithm.
A significant portion of the project was spent gaining the requisite background knowledge
in order to be able to attempt an implementation.
If more time was available for this implementation then through the use of parameter tuning,
the further development of the exploitability calculation algorithm and additional training
time would almost certainly result in vastly improved results.

I feel that the use of the python programming language for this project was a good choice,
its concise syntax, extensive set of support libraries and easy to use data structures
made the implementation process fast and efficient.
The same can be said for Qt Designer and PyQt when implementing the prototype game.
These libraries aided in facilitated the fast and easy development of a user interface despite the fact
that I had no previous experience in these technologies.

\section{Future Work}\label{sec:futureWork}

After this small step the next large task would be to tackle a more complex version of the game.
In this second iteration of the project limit texas hold'em would be tackled with the end goal of recreating
the results shown by Heinrich in his PhD thesis\citep{heinrich2017reinforcement}.
In this case we would measure win rate against other texas hold'em agents in order to evaluate
the success or failure of our product.

The final step of this project would be to attempt to extend the same methods to no-limit texas hold'em.
This game has a much larger state space(circa. $10^{164}$) than limit texas hold'em(circa. $10^{17}$).
This final approach would likely require the introduction of neural networks that would provide accurate
generalisation between states in order to reduce the complexity of the problem.